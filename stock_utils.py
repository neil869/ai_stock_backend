# stock_utils.py
import pandas as pd
import numpy as np
import time
import warnings
import logging
from datetime import datetime
import os
import pickle

# é…ç½®logging
logger = logging.getLogger(__name__)

# è®¾ç½®akshareè¯·æ±‚å¤´
os.environ['AKSHARE_HEADERS'] = '{"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"}'
warnings.filterwarnings('ignore')

try:
    import akshare as ak
    from snownlp import SnowNLP
    import jieba
except ImportError as e:
    raise RuntimeError(f"Missing dependency: {e}")

# ç¼“å­˜æ–‡ä»¶è·¯å¾„å®šä¹‰
STOCKS_CACHE_FILE = 'stocks_cache.pkl'

# å…¨å±€è‚¡ç¥¨åˆ—è¡¨ç¼“å­˜
_stocks_cache = None
_last_update_date = None

# å®šæ—¶ä»»åŠ¡æ ‡å¿—
_stocks_refreshing = False


# ==============================
# ðŸ“ ç¼“å­˜æœ¬åœ°æŒä¹…åŒ–åŠŸèƒ½
# ==============================
def load_stocks_cache():
    """
    ä»Žæœ¬åœ°æ–‡ä»¶åŠ è½½è‚¡ç¥¨åˆ—è¡¨ç¼“å­˜
    """
    global _stocks_cache, _last_update_date
    try:
        if os.path.exists(STOCKS_CACHE_FILE):
            with open(STOCKS_CACHE_FILE, 'rb') as f:
                cache_data = pickle.load(f)
                _stocks_cache = cache_data['stocks']
                _last_update_date = cache_data['last_update']
                logger.info(f"ä»Žæœ¬åœ°ç¼“å­˜åŠ è½½è‚¡ç¥¨åˆ—è¡¨æˆåŠŸï¼Œå…± {len(_stocks_cache)} æ¡æ•°æ®ï¼Œæœ€åŽæ›´æ–°æ—¥æœŸï¼š{_last_update_date}")
                return True
    except Exception as e:
        logger.error(f"åŠ è½½æœ¬åœ°è‚¡ç¥¨åˆ—è¡¨ç¼“å­˜å¤±è´¥ï¼š{e}")
    return False


def save_stocks_cache():
    """
    å°†è‚¡ç¥¨åˆ—è¡¨ç¼“å­˜ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
    """
    global _stocks_cache, _last_update_date
    try:
        if _stocks_cache is not None and _last_update_date is not None:
            cache_data = {
                'stocks': _stocks_cache,
                'last_update': _last_update_date
            }
            with open(STOCKS_CACHE_FILE, 'wb') as f:
                pickle.dump(cache_data, f)
            logger.info(f"è‚¡ç¥¨åˆ—è¡¨ç¼“å­˜å·²ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶ï¼š{STOCKS_CACHE_FILE}")
            return True
    except Exception as e:
        logger.error(f"ä¿å­˜è‚¡ç¥¨åˆ—è¡¨ç¼“å­˜åˆ°æœ¬åœ°å¤±è´¥ï¼š{e}")
    return False


# ============================== 
# ðŸ“Š å·¥å…·å‡½æ•°
# ==============================
def get_market_board(symbol: str) -> str:
    if symbol.startswith('688'):
        return 'ç§‘åˆ›æ¿'
    elif symbol.startswith('300'):
        return 'åˆ›ä¸šæ¿'
    else:
        return 'ä¸»æ¿'


def get_all_stocks(force_refresh=False):
    """
    èŽ·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨åˆ—è¡¨
    - force_refresh: æ˜¯å¦å¼ºåˆ¶åˆ·æ–°ç¼“å­˜
    """
    global _stocks_cache, _last_update_date
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°ç¼“å­˜ï¼ˆæ¯å¤©æ›´æ–°ä¸€æ¬¡ï¼‰
    current_date = datetime.now().date()
    if _stocks_cache is not None and not force_refresh and _last_update_date == current_date:
        return _stocks_cache.copy()
    
    try:
        # èŽ·å–è‚¡ç¥¨æ•°æ®
        logger.info("å¼€å§‹èŽ·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨åˆ—è¡¨...")
        df = ak.stock_info_a_code_name()
        logger.info(f"èŽ·å–åˆ° {len(df)} æ¡è‚¡ç¥¨æ•°æ®")
        # ç­›é€‰Aè‚¡è‚¡ç¥¨ï¼ˆä»£ç æ ¼å¼ï¼š6ä½æ•°å­—ï¼Œå‰ç¼€ä¸º0ã€3ã€6ï¼‰
        df = df[df['code'].str.match(r'^[036]\d{5}$')]
        logger.info(f"ç­›é€‰åŽ {len(df)} æ¡è‚¡ç¥¨æ•°æ®")
        
        # è¿‡æ»¤æŽ‰STã€é€€å¸‚ã€Bè‚¡ç­‰ç‰¹æ®Šè‚¡ç¥¨
        df = df[~df['name'].str.contains('ST|é€€|B', case=False, na=False)]
        logger.info(f"è¿‡æ»¤åŽ {len(df)} æ¡è‚¡ç¥¨æ•°æ®")   
        
        # æ›´æ–°ç¼“å­˜
        _stocks_cache = df
        _last_update_date = current_date
        
        # ä¿å­˜åˆ°æœ¬åœ°æ–‡ä»¶
        save_stocks_cache()
        
        logger.info(f"ç¼“å­˜æ›´æ–°å®Œæˆï¼Œå…± {len(df)} æ¡æœ‰æ•ˆè‚¡ç¥¨æ•°æ®")
        
        return df.copy()
    except Exception as e:
        # å¦‚æžœèŽ·å–å¤±è´¥ä½†æœ‰ç¼“å­˜ï¼Œè¿”å›žç¼“å­˜æ•°æ®
        if _stocks_cache is not None:
            logger.warning(f"èŽ·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥ï¼Œä½†è¿”å›žç¼“å­˜æ•°æ®ï¼š{e}") 
            return _stocks_cache.copy()
        raise RuntimeError(f"Failed to fetch stock list: {e}")


def get_guba_posts(symbol: str, pages=2):
    try:
        df = ak.stock_guba_em(symbol=symbol)
        if df.empty:
            return []
        df = df.sort_values('read_count', ascending=False).head(pages * 20)
        posts = (df['title'].fillna('') + 'ã€‚' + df['content'].fillna('')).tolist()
        return [p for p in posts if len(p) > 10]
    except Exception:
        return []


def basic_sentiment_score(text: str) -> float:
    try:
        s = SnowNLP(text)
        return s.sentiments * 2 - 1
    except:
        return 0.0


def analyze_stock_sentiment(symbol: str) -> dict:
    posts = get_guba_posts(symbol, pages=2)
    if not posts:
        return {"score": 0.0, "label": "â“ æ— æ•°æ®"}
    
    scores = [basic_sentiment_score(p) for p in posts[:30]]
    avg_score = np.mean(scores) if scores else 0.0
    
    if avg_score > 0.3:
        label = "ðŸ”¥ çœ‹æ¶¨"
    elif avg_score < -0.2:
        label = "â„ï¸ çœ‹è·Œ"
    else:
        label = "ðŸ˜ ä¸­æ€§"
    
    return {"score": round(avg_score, 3), "label": label}
